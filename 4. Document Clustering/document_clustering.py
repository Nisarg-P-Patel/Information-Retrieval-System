# -*- coding: utf-8 -*-
"""Document Clustering.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1GqsV1UfEL86b5IBMVKtgfJq0YL5A2A08

# AIM :- Document Clustering

Subtask:-
    1. Understand k means clustering
    2. Represent your corpus in TF-IDF vector form.
    3. Apply k means clustering algorithm on document vectors, and form clusters among them.
    4. Use cosine similarity to calculate distance between two vectors.

# Packages Used
"""

# Importing necessary libraries

import nltk
from nltk.corpus import stopwords
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.cm as cm
from sklearn.cluster import MiniBatchKMeans

"""# Importing data from jason file"""

df = pd.read_json('Department of Justice 2009-2018 Press Releases.json', lines=True)
df.head()

df.info()

"""# Data Pre-processing"""

def toLower(sentence):
    return sentence.lower()

def tokenizer(sentence):
    tokens = list(set(nltk.word_tokenize(sentence)))
    return tokens

def stopwords_removal(tokens):
    stop_words = nltk.corpus.stopwords.words('english')
    stop_words.extend([',','?','""',"''",'.','!', "'",'"',"'d","'ll",'[',']','--',':',';','///','@', '``',
                       '#', '$', '%', '&', "'re", "'s", '(', ')', '*', '**', '**the', '-', '/', '//',
                       '§', '§§','...','–', '—', '‘', '’', '“', '”', '•', '─',"'m", "'ve", '***'])
    filtered_tokens = [i for i in tokens if not i in stop_words]
    return filtered_tokens

def stemming(tokens):
    stemmer = nltk.stem.porter.PorterStemmer()
    stemmed_tokens = [stemmer.stem(i) for i in tokens]
    return stemmed_tokens

def pre_process(text):
    tokens = [word for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]
    tokens = stopwords_removal(tokens)
    stems = stemming(tokens)
    return stems

"""# Creating TF-IDF vector"""

#define vectorizer parameters
tfidf_vectorizer = TfidfVectorizer(max_features=15000,min_df = 5,max_df = 0.95,tokenizer=pre_process)

tfidf_matrix = tfidf_vectorizer.fit_transform(df.contents[:10000]) #fit the vectorizer to synopses

print(tfidf_matrix.shape)

terms = tfidf_vectorizer.get_feature_names()
print(terms[5000:5100])

"""# Implementing ELBOW METHOD"""

# the elbow method

inertia_value = []
for k in range(3,20,2):
    inertia_value.append(MiniBatchKMeans(n_clusters=k, init_size=1024, batch_size=2048, random_state=20).fit(tfidf_matrix).inertia_)
f, ax = plt.subplots(1, 1)
ax.plot(range(3,20,2), inertia_value, marker='*')
ax.set_xlabel('Cluster Centers')
ax.set_xticks(range(3,20,2))
ax.set_xticklabels(range(3,20,2))
ax.set_ylabel('inertia_value')
ax.set_title('The elbow method')

clusters = MiniBatchKMeans(n_clusters=13, init_size=1024, batch_size=2048, random_state=20).fit_predict(tfidf_matrix)

"""# Fetching fisrt 15 elements of each corpus"""

df = pd.DataFrame(tfidf_matrix.todense()).groupby(clusters).mean()
wc=[]
for i,r in df.iterrows():
    print('\nCluster {}'.format(i+1))
    print(','.join([terms[t] for t in np.argsort(r)[-15:]]))
    wc.append([terms[t] for t in np.argsort(r)[-15:]])

"""# Visulizing the cluster using word-cloud"""

import matplotlib.pyplot as plt
from wordcloud import WordCloud

#  plot Numerical Data
a = 4  # number of rows
b = 3  # number of columns
c = 1  # initialize plot counter

fig = plt.figure(figsize=(22,22))

for i in range(a):
    plt.subplot(a, b, c)
    wordcloud = WordCloud(background_color ='black', min_font_size = 15,colormap='Set2').generate(str(wc[i*b]))
    plt.imshow(wordcloud)
    plt.title('Cluster {}'.format(i*b+1))
    plt.axis("off")
    c = c + 1

    plt.subplot(a, b, c)
    wordcloud = WordCloud(background_color ='black', min_font_size = 15,colormap='Set2').generate(str(wc[i*b+1]))
    plt.imshow(wordcloud)
    plt.title('Cluster {}'.format(i*b+2))
    plt.axis("off")
    c = c + 1

    plt.subplot(a, b, c)
    wordcloud = WordCloud(background_color ='black', min_font_size = 15,colormap='Set2').generate(str(wc[i*b+2]))
    plt.imshow(wordcloud)
    plt.title('Cluster {}'.format(i*b+3))
    plt.axis("off")
    c = c + 1

plt.show()

"""# Finding Cosine Similarity"""

from sklearn.metrics.pairwise import cosine_similarity

sim = cosine_similarity(df,df)

df = pd.DataFrame(sim)
df

"""# Learning Outcomes

1. Get to know the practical implementation of K-Means Clustering in text data
2. Get to know the importnace of the filtering process for the corpus and methodology
3. Get to know what is cosine similarity,term frquency,tf-idf and their pros&cons.
"""

