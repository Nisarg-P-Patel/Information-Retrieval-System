# -*- coding: utf-8 -*-
"""Dimensionality Reduction using Singular Value Decomposition.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/17ZBqOAdroNS7hJKA3f1KRb22UX-91eN1

# AIM :- Singular Value Decomposition

Dimensionality reduction using Singular Value Decomposition.

# Packages Used
"""

# Importing necessary libraries

import nltk
from nltk.corpus import stopwords
from sklearn.feature_extraction.text import TfidfVectorizer
from scipy.sparse.linalg import svds
import pandas as pd
import numpy as np
from sklearn.decomposition import TruncatedSVD
from sklearn.metrics.pairwise import cosine_similarity as cs

"""# Importing data from json file"""

df = pd.read_json('Department of Justice 2009-2018 Press Releases.json', lines=True)
df.head()

df.info()

"""# Data Pre-processing"""

def toLower(sentence):
    return sentence.lower()

def tokenizer(sentence):
    tokens = list(set(nltk.word_tokenize(sentence)))
    return tokens

def stopwords_removal(tokens):
    stop_words = nltk.corpus.stopwords.words('english')
    stop_words.extend([',','?','""',"''",'.','!', "'",'"',"'d","'ll",'[',']','--',':',';','///','@', '``',
                       '#', '$', '%', '&', "'re", "'s", '(', ')', '*', '**', '**the', '-', '/', '//',
                       '§', '§§','...','–', '—', '‘', '’', '“', '”', '•', '─',"'m", "'ve", '***'])
    filtered_tokens = [i for i in tokens if not i in stop_words]
    return filtered_tokens

def stemming(tokens):
    stemmer = nltk.stem.porter.PorterStemmer()
    stemmed_tokens = [stemmer.stem(i) for i in tokens]
    return stemmed_tokens

def pre_process(text):
    tokens = [word for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]
    tokens = stopwords_removal(tokens)
    stems = stemming(tokens)
    return stems

"""# Creating TF-IDF vector"""

#define vectorizer parameters
tfidf_vectorizer = TfidfVectorizer(max_features=1000,min_df = 5,max_df = 0.95,tokenizer=pre_process)

tfidf_matrix = tfidf_vectorizer.fit_transform(df.contents[:1000]) #fit the vectorizer to synopses

print(tfidf_matrix.shape)

terms = tfidf_vectorizer.get_feature_names()
print(terms[500:600])

"""# Implementing SVD for dimension reduction"""

print(type(tfidf_matrix))

A = tfidf_matrix[:]
print(A.toarray())
print(A.shape)

U, s, VT = svds(A,k=500)
print(U)
print()
print(s)
print()
print(VT)

print(U.shape,s.shape,VT.shape)

svd = TruncatedSVD(n_components=500)
svd.fit(A)
result = svd.transform(A)
print(result)

print(result.shape)

"""# Concept Space"""

# Concept space
k=5

U_k = U[:,:k]
V_k = VT[:k,:]
result_k = result[:k,:k]

print(U_k.shape)
print(V_k.shape)
print(result_k.shape)

query = ['Department of Justice']
query_matrix = tfidf_vectorizer.transform(query).toarray()
print(query_matrix.shape)
# print(query_matrix)

inverse = np.linalg.inv(result_k)
print(inverse.shape)
q_t = query_matrix[:]
q_k = np.dot(np.dot(q_t,U_k),inverse)
print(q_k.shape)

V_k_final = np.transpose(V_k)
print(V_k_final.shape)

similarity = []
for x in range(1000):
    similarity.extend(cs([V_k_final[x]],q_k))

similarity.sort()
print('Highest similarity',similarity[-1])
print('Lowest similarity',similarity[0])

"""# Learning Outcomes

1. In this practical, we learned how we can use Dimensality reduction using SVD
    2. We learned to turn matrix into lower dimension using soncept space
    3. Get to know that TF-IDF gives sparse metrix
    4. Get to know the working of SVD
    5. Using dimnesionality reduction techniques like PCA, SVD we can increase performance of our IR system
    6. Through SVD performnce based on retrieval time is increased by nearly 4 times compared to Tf-IDF matrix.
"""

